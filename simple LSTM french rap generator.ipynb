{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRgoKraSep37"
   },
   "source": [
    "# 2 - Word-level LSTM french rap lyrics generator\n",
    "\n",
    "We start by a simple  word-level LSTM french rap lyrics generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163557,
     "status": "ok",
     "timestamp": 1617711797322,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "GlCVXz_cuUHL",
    "outputId": "120af3ca-0501-4597-db4a-88653e616b70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34218,
     "status": "ok",
     "timestamp": 1617711832925,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "pzWB4fhHecJv",
    "outputId": "2a4ad663-ca76-44a8-b4c3-03bd052ab7fd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:32<00:00,  1.64it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "the first step is to read the lyric corpus,\n",
    "removing the ponctuation and spliting it into words\n",
    "\n",
    "We used a reduced list of artists contained in lyricsbis as we just had too much lyrics for our computing power...\n",
    "'''\n",
    "\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "import os\n",
    "\n",
    "lyrics_path = '/content/drive/MyDrive/ProjetDL/lyrics/*.txt'\n",
    "lyrics_path_bis = '/content/drive/MyDrive/ProjetDL/lyricsbis/*.txt'\n",
    "\n",
    "\n",
    "files = glob.glob(lyrics_path_bis)\n",
    "corpus = []\n",
    "for file in tqdm(files, position=0, leave=True) :\n",
    "    with open(file) as f :\n",
    "        text = f.read().lower().replace('\\n', ' \\n ')\n",
    "        text_in_words = [w for w in text.split(' ') if w.strip() != '' or w == '\\n']\n",
    "        corpus = corpus + text_in_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 873,
     "status": "ok",
     "timestamp": 1617711870005,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "gQLpxxqyjf4q",
    "outputId": "866ab3f5-a522-42d3-fc48-4f1a1b2ada83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4309886 words in the corpus\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(corpus),\"words in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1617711875156,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "OuvcB5y7jAJK",
    "outputId": "20483ea8-b49b-49b0-eea0-c5052000d590"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words before ignoring: 215562\n",
      "Ignoring words with frequency < 10\n",
      "Unique words after ignoring: 25680\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Now we compute the word frequency of our corpus\n",
    "and we remove the words that appear less than a \n",
    "chosen threshold : they will be of no interest\n",
    "for the learning of the algorithm\n",
    "'''\n",
    "\n",
    "word_freq = {}\n",
    "for word in corpus:\n",
    "    word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "MIN_WORD_FREQUENCY = 10 # this is already huge, but we have a lot of words...\n",
    "\n",
    "ignored_words = set()\n",
    "for k, v in word_freq.items():\n",
    "    if word_freq[k] < MIN_WORD_FREQUENCY:\n",
    "        ignored_words.add(k)\n",
    "\n",
    "words = set(corpus)\n",
    "print('Unique words before ignoring:', len(words))\n",
    "print('Ignoring words with frequency <', MIN_WORD_FREQUENCY)\n",
    "words = sorted(set(words) - ignored_words)\n",
    "print('Unique words after ignoring:', len(words))\n",
    "\n",
    "word_indices = dict((c, i) for i, c in enumerate(words))\n",
    "indices_word = dict((i, c) for i, c in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4325,
     "status": "ok",
     "timestamp": 1617711882405,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "38f1GnP7jvgj",
    "outputId": "e5a941c0-b6af-4cd9-ff07-3397d40bd026"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1077468/1077468 [00:03<00:00, 284537.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored sequences: 736988\n",
      "Remaining sequences: 340480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "We will teach our Neural network to predict the next word\n",
    "given a sequence of previous word, of a chosen length.\n",
    "'''\n",
    "STEP = 4 # we only pick one word every four other\n",
    "sentences = []\n",
    "next_words = []\n",
    "ignored = 0\n",
    "\n",
    "SEQUENCE_LEN = 15\n",
    "for i in tqdm(range(0, len(corpus) - SEQUENCE_LEN, STEP), position=0, leave=True):\n",
    "    # Only add sequences where no word is in ignored_words\n",
    "    if len(set(corpus[i: i+SEQUENCE_LEN+1]).intersection(ignored_words)) == 0:\n",
    "        sentences.append(corpus[i: i + SEQUENCE_LEN])\n",
    "        next_words.append(corpus[i + SEQUENCE_LEN])\n",
    "    else:\n",
    "        ignored = ignored+1\n",
    "print('Ignored sequences:', ignored)\n",
    "print('Remaining sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1901,
     "status": "ok",
     "timestamp": 1617711888907,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "r39CyJjNkdOy",
    "outputId": "4fe56d5b-9c92-4f20-d963-46d6fa465209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling sentences\n",
      "Size of training set = 333670\n",
      "Size of test set = 6810\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def shuffle_and_split_training_set(sentences_original, next_original, percentage_test=2):\n",
    "    # shuffle at unison\n",
    "    print('Shuffling sentences')\n",
    "\n",
    "    tmp_sentences = []\n",
    "    tmp_next_word = []\n",
    "    for i in np.random.permutation(len(sentences_original)):\n",
    "        tmp_sentences.append(sentences_original[i])\n",
    "        tmp_next_word.append(next_original[i])\n",
    "\n",
    "    cut_index = int(len(sentences_original) * (1.-(percentage_test/100.)))\n",
    "    x_train, x_test = tmp_sentences[:cut_index], tmp_sentences[cut_index:]\n",
    "    y_train, y_test = tmp_next_word[:cut_index], tmp_next_word[cut_index:]\n",
    "\n",
    "    print(\"Size of training set = %d\" % len(x_train))\n",
    "    print(\"Size of test set = %d\" % len(y_test))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(sentences_train, next_words_train), (sentences_test, next_words_test) = shuffle_and_split_training_set(sentences, next_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8890,
     "status": "ok",
     "timestamp": 1617711905816,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "yYsuvZgkzFPf",
    "outputId": "b3a87ded-943a-435b-942f-67e7f8af6cb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 512)         13148160  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               656384    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 25680)             6599760   \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 25680)             0         \n",
      "=================================================================\n",
      "Total params: 20,404,304\n",
      "Trainable params: 20,404,304\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping           \n",
    "from keras.models import Sequential                                                  \n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Embedding  \n",
    "\n",
    "dropout = 0.2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(words), output_dim=512))\n",
    "model.add(Bidirectional(LSTM(128)))\n",
    "if dropout > 0:\n",
    "    model.add(Dropout(dropout))\n",
    "model.add(Dense(len(words)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "                optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7a4ChN2fljZu"
   },
   "outputs": [],
   "source": [
    "# we need data generators to feed the model\n",
    "# otherwise it would cause a memory error\n",
    "import numpy as np\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from operator import itemgetter\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def generator(sentence_list, next_word_list, batch_size):\n",
    "    index = 0\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, SEQUENCE_LEN), dtype=np.int32)\n",
    "        y = np.zeros((batch_size), dtype=np.int32)\n",
    "        for i in range(batch_size):\n",
    "            for t, w in enumerate(sentence_list[index % len(sentence_list)]):\n",
    "                x[i, t] = word_indices[w]\n",
    "            y[i] = word_indices[next_word_list[index % len(sentence_list)]]\n",
    "            index = index + 1\n",
    "        yield x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "elapsed": 4767528,
     "status": "error",
     "timestamp": 1617716692314,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "0qG8pTWMm4KN",
    "outputId": "c5bddeea-0766-4e17-ba49-ea911906ef35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10641/10641 [==============================] - 1566s 144ms/step - loss: 2.1423 - accuracy: 0.6053 - val_loss: 1.5122 - val_accuracy: 0.7252\n",
      "Epoch 2/100\n",
      "10641/10641 [==============================] - 1566s 147ms/step - loss: 1.8265 - accuracy: 0.6406 - val_loss: 1.1645 - val_accuracy: 0.7679\n",
      "Epoch 3/100\n",
      "10641/10641 [==============================] - 1543s 145ms/step - loss: 1.6052 - accuracy: 0.6673 - val_loss: 0.9841 - val_accuracy: 0.7962\n",
      "Epoch 4/100\n",
      "  556/10641 [>.............................] - ETA: 24:43 - loss: 1.3644 - accuracy: 0.7099"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-8c349d85351b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_words_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                         validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "We use a generator function and a sparse representation of the labels to train our model\n",
    "This is a great way to gain time.\n",
    "sparse representation => sparse_categorical_crossentropy as loss function\n",
    "'''\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "#file_path = '/content/drive/MyDrive/ProjetDL/Modelbis'     #Changer le nom avant\n",
    "model_checkpoint = ModelCheckpoint(filepath=file_path + '.hdf5', monitor='val_accuracy',\n",
    "                                                   save_best_only=True)\n",
    "\n",
    "callbacks_list = [early_stopping,model_checkpoint]\n",
    "\n",
    "model.fit(generator(sentences, next_words, BATCH_SIZE),\n",
    "                        steps_per_epoch=int(len(sentences)/BATCH_SIZE) + 1,\n",
    "                        epochs=100,\n",
    "                        callbacks=callbacks_list,\n",
    "                        validation_data=generator(sentences_test, next_words_test, BATCH_SIZE),\n",
    "                        validation_steps=int(len(sentences_test)/BATCH_SIZE) + 1)\n",
    "\n",
    "\n",
    "'''\n",
    "usually, it is google cloud who choose when the training stop... \n",
    "but thanks to our callback function we can start to retrain the saved model\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oi6KvR_B0VOY"
   },
   "outputs": [],
   "source": [
    "# if a trained model already exists, we can load it here, then execute the above cell\n",
    "\n",
    "file_path = '/content/drive/MyDrive/ProjetDL/Modelbis'\n",
    "#Faut execute la cell de la création du modèle plus haut avant\n",
    "model.load_weights(file_path + '.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27259,
     "status": "ok",
     "timestamp": 1617719263933,
     "user": {
      "displayName": "Felix Hubert",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgwHrgigBFYRaGbI1CBVTsXY4s7lh2p9gaVyUuQ7Q=s64",
      "userId": "01606205728076468550"
     },
     "user_tz": -120
    },
    "id": "Fb2DmGY_amTU",
    "outputId": "58e727fd-c0ce-4673-dc0e-ac0df31f8d7a",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mes', 'textes', 'sont', 'écrits', 'par', 'une', 'machine', '\\n', 'mon', 'sons', 'vient', 'de', 'la', 'rue', 'gros']\n",
      "\n",
      "generation pour température 0.1 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "en un peu de moi \n",
      " temps de temps pour les gens \n",
      " en moi je sais que je me sens beaucoup \n",
      " mais quand mon équipe est bonne si je me veux \n",
      " là où je me sens seul seul au pire \n",
      " si tu peux faire plus de respect, que dieu nous a seul \n",
      " a plus de gens qui me check \n",
      " les gens sont pas les gens les gens sont trop\n",
      "\n",
      "generation pour température 0.2 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "un peu de ce qui est en \n",
      " a quoi le choix de la vie \n",
      " en moins de la vie des de rêve \n",
      " et le mal à dire \n",
      " et on est très mal et on fait mal \n",
      " on est tout pour ça pour tout on fait tout \n",
      " on fait tout seul, tout le monde \n",
      " tout le monde est mort \n",
      " tout le monde est prêt à la maison \n",
      "\n",
      "\n",
      "generation pour température 0.3 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "en rêve \n",
      " j'ai mis mon temps de moi je sais que je suis \n",
      " je sais que j'ai mis mes yeux sur mes larmes \n",
      " je suis \n",
      " j'ai voulu faire des larmes \n",
      " j'ai fait le tour dans le crâne \n",
      " le quartier le nord \n",
      " dans le quartier qu'on est dans le ghetto \n",
      " pas le temps de envier \n",
      " et les gars qui ne vis pas, j'en ai plein \n",
      " donc\n",
      "\n",
      "generation pour température 0.4 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "un peu de ce qui est ? \n",
      " il a perdu de la merde à la merde \n",
      " à qui faire prendre des sous et on se fait du place \n",
      " on fait fait prendre dans le vide \n",
      " le son le fait le place \n",
      " le son est le vide \n",
      " le son est le début \n",
      " le meilleur pote le monde est en \n",
      " le mal que le père est en ville \n",
      "\n",
      "\n",
      "generation pour température 0.5 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "un tôt en petit \n",
      " qui veulent des jeunes qui font des gros bâtard \n",
      " et mes gars de gros cul de ma vie \n",
      " si t'as la même si tu m'aimes pas, on t'a pas vu \n",
      " mais on fait la même chose est tout \n",
      " on fait des des choses à la gens la croire et la vie \n",
      " on vise la vie beaucoup de gens \n",
      " beaucoup de choses la misère qui\n",
      "\n",
      "generation pour température 0.6 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "désordre et en la france et prends pas la pour soigner vos ses trouve \n",
      " en bas sous les putes , \n",
      " pas les gens qui me trouve au fond \n",
      " le daron qui baise ? \n",
      " j'sais pas pourquoi faire semblant \n",
      " j'sais plus où t'as te l'air devant \n",
      " - bah ouais \n",
      " mais j'ai plus de plus la gueule \n",
      " , \n",
      " plus de plus la gueule à chaud \n",
      " on est\n",
      "\n",
      "generation pour température 0.7 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "minute de vies et de mes fesses \n",
      " mon époque est trop doué \n",
      " jeunes c'est ça revient sous le bâtiment \n",
      " eh, eh, entre le sinon \n",
      " je partirai comme à mon mec \n",
      " à part le mer, je mets de la jungle à moi \n",
      " peu importe à moitié de frères \n",
      " moi je me dis rien, moi \n",
      " moi il a fait du sale et moi je me dis \n",
      " j'ai rien\n",
      "\n",
      "generation pour température 0.9 :\n",
      "\n",
      "mes textes sont écrits par une machine \n",
      " mon sons vient de la rue gros\n",
      "chaque jour que je vous pleure, un for en blancs \n",
      " mes mots pour leurs rien se gars, un est un a a ma porte \n",
      " il y a pas de c'qui faut \n",
      " c'est des qu'elle avait la loi du sur y a \n",
      " et c'est comme moi c'est comme ça qu'un c'est comme ça \n",
      " y a les gens, comme ça \n",
      " on passe faut pas \n",
      " on se envier \n",
      " et qu'en\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here we generate lyrics for different temperatures of the LSTM.\n",
    "\n",
    "seed_index = np.random.randint(len(sentences))\n",
    "seed= ['mes','textes','sont','écrits','par','une','machine','\\n','mon','sons','vient','de','la','rue','gros']\n",
    "#seed = (sentences)[seed_index]\n",
    "print(seed)\n",
    "print('')\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "for diversity in [0.1,0.2,0.3, 0.4, 0.5, 0.6, 0.7,0.9]:\n",
    "    sentence = seed\n",
    "\n",
    "    lyric = []\n",
    "    for i in range(75):\n",
    "        x_pred = np.zeros((1, SEQUENCE_LEN))\n",
    "        for t, word in enumerate(sentence):\n",
    "            x_pred[0, t] = word_indices[word]\n",
    "\n",
    "        preds = model.predict(x_pred, verbose=0)[0] # reconstructed_model or model\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_word = indices_word[next_index]\n",
    "\n",
    "        sentence = sentence[1:]\n",
    "        sentence.append(next_word)\n",
    "\n",
    "        lyric.append(next_word)\n",
    "    print('generation pour température',diversity,':')\n",
    "    print('')\n",
    "    print(' '.join(seed))\n",
    "    print(' '.join(lyric))\n",
    "    print('')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Rap Generator V0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
